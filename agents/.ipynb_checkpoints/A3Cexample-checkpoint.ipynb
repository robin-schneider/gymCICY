{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple example using A3C\n",
    "we begin by importing some packages; see also https://github.com/chainer/chainerrl/blob/master/examples/atari/a3c/train_a3c.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainerrl\n",
    "from chainerrl.agents import a3c\n",
    "from chainerrl import experiments\n",
    "import chainer.links as L\n",
    "from chainerrl import misc\n",
    "from chainerrl.optimizers.nonbias_weight_decay import NonbiasWeightDecay\n",
    "from chainerrl.optimizers import rmsprop_async\n",
    "from chainerrl import policy\n",
    "from chainerrl import v_function\n",
    "\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from pyCICY import CICY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the CICY we want to study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = CICY([[1,0, 1, 1], [1,0, 1, 1], [1,1, 1, 0], [1,1, 1, 0], [1,1, 0, 1], [1,1, 0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to load the gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.register(\n",
    "     id='CICY-v0',\n",
    "     entry_point='gym_CICYlbmodels.envs.env_flip_4:flip_4',\n",
    "    kwargs={'M': M, 'r': 2, 'max': 2},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CICY-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box(5, 6)\n",
      "action space: Discrete(48)\n",
      "initial observation: [[ 0  1  0  0  1  0]\n",
      " [ 1  1 -1  1 -1  1]\n",
      " [ 1 -1 -1  1  0 -1]\n",
      " [ 0  0  0 -1  0 -1]\n",
      " [-2 -1  2 -1  0  1]]\n",
      "next observation: [[ 0  1  0  0  1  0]\n",
      " [ 1  1 -1  1 -1  1]\n",
      " [ 1 -1 -1  1  0 -1]\n",
      " [ 0  0 -1 -1  0 -1]\n",
      " [-2 -1  3 -1  0  1]]\n",
      "reward: -0.6000000000000001\n",
      "done: False\n",
      "info: {}\n"
     ]
    }
   ],
   "source": [
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "obs = env.reset()\n",
    "print('initial observation:', obs)\n",
    "\n",
    "action = env.action_space.sample()\n",
    "obs, r, done, info = env.step(action)\n",
    "print('next observation:', obs)\n",
    "print('reward:', r)\n",
    "print('done:', done)\n",
    "print('info:', info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.0\n",
    "lr = 7e-4\n",
    "eps = 1e-2\n",
    "gamma = 0.999\n",
    "beta = 1e-2\n",
    "alpha = 0.99\n",
    "obs_size = 5*M.len\n",
    "n_actions = env.action_space.n\n",
    "t_max = 5\n",
    "nsteps = 1000\n",
    "eval_n_steps = 5*10**4\n",
    "eval_n_episodes = 2\n",
    "eval_interval = 50000\n",
    "max_episode_len = 10000\n",
    "processes = 4\n",
    "outdir = 'data_a3c'\n",
    "seed = 1\n",
    "process_seeds = np.arange(processes) + seed * processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define action value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QFunction(chainer.Chain):\n",
    "\n",
    "    def __init__(self, obs_size, n_output_channels, n_hidden_channels=50):\n",
    "        \n",
    "        self.n_output_channels = n_output_channels\n",
    "        self.n_input_channels = obs_size\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        with self.init_scope():\n",
    "            self.l0 = L.Linear(obs_size, n_hidden_channels)\n",
    "            self.l1 = L.Linear(n_hidden_channels, n_hidden_channels)\n",
    "            self.l2 = L.Linear(n_hidden_channels, n_hidden_channels)\n",
    "            self.l3 = L.Linear(n_hidden_channels, n_output_channels)\n",
    "            #self.l4 = L.Linear(n_hidden_channels+150, n_actions)\n",
    "\n",
    "    def __call__(self, x, test=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (ndarray or chainer.Variable): An observation\n",
    "            test (bool): a flag indicating whether it is in test mode\n",
    "        \"\"\"\n",
    "        h = F.tanh(self.l0(x))\n",
    "        h = F.tanh(self.l1(h))\n",
    "        h = F.tanh(self.l2(h))\n",
    "        #h = F.tanh(self.l3(h))\n",
    "        #chainerrl.action_value.DiscreteActionValue(self.l4(h))\n",
    "        return F.tanh(self.l3(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3CFF(chainer.ChainList, a3c.A3CModel):\n",
    "\n",
    "    def __init__(self, n_input, n_actions, n_hidden):\n",
    "        self.head = QFunction(n_input, n_hidden)\n",
    "        self.pi = policy.FCSoftmaxPolicy(\n",
    "            self.head.n_output_channels, n_actions)\n",
    "        self.v = v_function.FCVFunction(self.head.n_output_channels)\n",
    "        super().__init__(self.head, self.pi, self.v)\n",
    "\n",
    "    def pi_and_v(self, state):\n",
    "        out = self.head(state)\n",
    "        return self.pi(out), self.v(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A3CFF(obs_size, n_actions, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the computational graph and save it in the output directory.\n",
    "fake_obs = chainer.Variable(np.zeros(obs_size, dtype=np.float32)[None], name='observation')\n",
    "with chainerrl.recurrent.state_reset(model):\n",
    "    # The state of the model is reset again after drawing the graph\n",
    "    chainerrl.misc.draw_computational_graph(\n",
    "            [model(fake_obs)],\n",
    "            os.path.join(outdir, 'model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = rmsprop_async.RMSpropAsync(lr=lr, eps=eps, alpha=alpha)\n",
    "opt.setup(model)\n",
    "opt.add_hook(chainer.optimizer.GradientClipping(40))\n",
    "if weight_decay > 0:\n",
    "    opt.add_hook(NonbiasWeightDecay(args.weight_decay))\n",
    "\n",
    "phi = lambda x: x.astype(np.float32, copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = a3c.A3C(model, opt, t_max=t_max, gamma=gamma,\n",
    "                    beta=beta, phi=phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(process_idx, test):\n",
    "    \n",
    "    process_seed = process_seeds[process_idx]\n",
    "    env_seed = 2 ** 31 - 1 - process_seed if test else process_seed\n",
    "    env = gym.make('CICY-v0')\n",
    "    env.seed(int(env_seed))\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n",
      "/home/robin/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n",
      "/home/robin/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n",
      "/home/robin/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "# Linearly decay the learning rate to zero\n",
    "def lr_setter(env, agent, value):\n",
    "    agent.optimizer.lr = value\n",
    "\n",
    "lr_decay_hook = experiments.LinearInterpolationHook(nsteps, lr, 0, lr_setter)\n",
    "\n",
    "training = experiments.train_agent_async(\n",
    "            agent=agent,\n",
    "            outdir=outdir,\n",
    "            processes=processes,\n",
    "            make_env=make_env,\n",
    "            profile=False,\n",
    "            steps=nsteps,\n",
    "            #eval_n_steps=50000,\n",
    "            eval_interval=eval_interval,\n",
    "            max_episode_len=max_episode_len,\n",
    "            global_step_hooks=[lr_decay_hook],\n",
    "            save_best_so_far_agent=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test episode: 0 R: 2937.199999999979\n",
      "test episode: 1 R: 2188.999999999962\n",
      "test episode: 2 R: 1502.5999999999194\n",
      "test episode: 3 R: 3492.400000000012\n",
      "test episode: 4 R: 1429.599999999941\n",
      "test episode: 5 R: 1807.5999999998871\n",
      "test episode: 6 R: 2165.3999999999905\n",
      "test episode: 7 R: 2667.0000000000537\n",
      "test episode: 8 R: 2194.2000000000176\n",
      "test episode: 9 R: 2658.3999999999714\n",
      "test episode: 10 R: 2776.8000000000293\n",
      "test episode: 11 R: 1977.3999999998925\n",
      "test episode: 12 R: 3122.6000000000304\n",
      "test episode: 13 R: 2649.8000000000247\n",
      "test episode: 14 R: 2047.9999999998986\n",
      "test episode: 15 R: 1022.9999999999559\n",
      "test episode: 16 R: 2473.9999999999245\n",
      "test episode: 17 R: 2210.5999999999544\n",
      "test episode: 18 R: 2704.4000000000237\n",
      "test episode: 19 R: 1976.9999999999243\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    R = 0\n",
    "    t = 0\n",
    "    while not done and t < 10000-1:\n",
    "        #env.render()\n",
    "        action = agent.act(obs)\n",
    "        obs, r, done, _ = env.step(action)\n",
    "        R += r\n",
    "        t += 1\n",
    "    print('test episode:', i, 'R:', R)\n",
    "    agent.stop_episode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a random walker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test episode: 0 R: 2209.7999999999483\n",
      "test episode: 1 R: 1596.199999999934\n",
      "test episode: 2 R: 2896.199999999987\n",
      "test episode: 3 R: 995.9999999999548\n",
      "test episode: 4 R: 2710.200000000023\n",
      "test episode: 5 R: 1249.999999999951\n",
      "test episode: 6 R: 2679.799999999957\n",
      "test episode: 7 R: 1879.9999999999177\n",
      "test episode: 8 R: 1755.59999999991\n",
      "test episode: 9 R: 1389.399999999928\n",
      "test episode: 10 R: 2108.1999999999266\n",
      "test episode: 11 R: 2351.800000000016\n",
      "test episode: 12 R: 2685.4000000000106\n",
      "test episode: 13 R: 2557.8000000000093\n",
      "test episode: 14 R: 3415.0000000000937\n",
      "test episode: 15 R: 2871.400000000014\n",
      "test episode: 16 R: 2448.1999999999553\n",
      "test episode: 17 R: 2150.599999999932\n",
      "test episode: 18 R: 1197.1999999999662\n",
      "test episode: 19 R: 2800.800000000037\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    R = 0\n",
    "    t = 0\n",
    "    while not done and t < 10000-1:\n",
    "        #env.render()\n",
    "        action = np.random.randint(2*4*M.len)\n",
    "        obs, r, done, _ = env.step(action)\n",
    "        R += r\n",
    "        t += 1\n",
    "    print('test episode:', i, 'R:', R)\n",
    "    #agent.stop_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_CICYlbmodels.envs.stack import create_stack, create_stack_h, _quick_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = create_stack(M, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2980"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in stack:\n",
    "    h = M.line_co(l)\n",
    "    if h[0] > 0:\n",
    "        print('failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
