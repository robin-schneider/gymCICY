{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "In this tutorial we will explore some of the gymCICY environments and construct a simple A3C agent using the ChainerRL library.\n",
    "\n",
    "We begin by importing some relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import gymCICY\n",
    "import os\n",
    "from pyCICY import CICY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The physical setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets pick a well studied example from the [literature](https://arxiv.org/pdf/1106.4804.pdf). Take the CICY with index 6777 from the [cicylist](http://www-thphys.physics.ox.ac.uk/projects/CalabiYau/cicylist/cicylist.txt). It has the following configuration matrix\n",
    "\n",
    "$$\n",
    "\t\\mathcal{M}_{6777} =  \\left[\n",
    "\t\\begin{array}{c||cccc}\n",
    "\t1 & 1 & 1 & 0 & 0 \\\\\n",
    "\t1 & 0 & 0 & 0 & 2 \\\\\n",
    "\t1 & 0 & 0 & 2 & 0 \\\\\n",
    "\t1 & 2 & 0 & 0 & 0 \\\\\n",
    "\t3 & 1 & 1 & 0 & 1\n",
    "\t\\end{array}\n",
    "\t\\right]^{5,37}_{-64}.\n",
    "$$\n",
    "\n",
    "We now define a CICY object using the pyCICY library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = np.array([[1,1,1,0,0], [1,0,0,0,2], [1,0,0,2,0], [1,2,0,0,0], [3,1,1,1,1]])\n",
    "M = CICY(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is known that the following vector bundle\n",
    "$$\n",
    "V = (1,0,0,-1,0) \\oplus (1,-1,-2,0,1) \\oplus (0,1,1,1,-1) \\oplus (0,-1,1,0,0) \\oplus (-2,1,0,0,0)\n",
    "$$\n",
    "leads to a string compactification with 'realistic' gauge group and particle content when using a $\\mathbb{Z}_2$ Wilson line. Next we will explore the f4p1 environment and try to recover this results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gymCICY\n",
    "\n",
    "As explained in [Branes with Brains](https://arxiv.org/abs/1903.11616) the agents, in this case us, interact with the gym environment by performing an action via\n",
    "\n",
    "```python\n",
    "obs, r, done, info = env.step(action)\n",
    "```\n",
    "\n",
    "The action leads to \n",
    "\n",
    "1. an observation, *obs*, which in our case is the sum of line bundles V. It is needed for the agent to determine its next action.\n",
    "2. a reward, *r*, judging the performed action. The reward is determined by how many model building constraints have been satisfied.\n",
    "3. a boolean, *done*. It is True if a model has been found and all constraints are satisfied, otherwise False. If True one has to reset the environment. \n",
    "4. and possibly some additional information, *info*. Here, we slightly break with usual gym notation and actually return information about the found model. If no model has been found this is an empty dict.\n",
    "\n",
    "We will next define the f4p1 gym environment and explore the setting in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by registering the environment. It requires three parameters from us.\n",
    "\n",
    "1. A CICY object $M$\n",
    "2. the rank $|\\Gamma|$ of the freely acting symmetry\n",
    "3. $q_{max}$ the maximal allowed charge for the first four line bundles.\n",
    "\n",
    "From before we have $q_{max}:=2$, $|\\Gamma| := 2$ and $M := \\mathcal{M}_{6777}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 2\n",
    "qmax = 2\n",
    "gym.envs.register(\n",
    "        id='CICY-v1',\n",
    "        entry_point='gymCICY.envs.f4p1:f4p1',\n",
    "        kwargs={'M': M, 'r': rank, 'max': qmax},\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the registration we are in a position to create an OpenAI environment object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CICY-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step when creating a new environment is to set a seed and reset the observation space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(2020)\n",
    "new_obs = env.reset()\n",
    "new_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the current vector bundle is\n",
    "$$\n",
    "V_{0} = (-1,-1,1,0,-1) \\oplus (0,-1,-1,-1,1) \\oplus (0,1,1,1,0) \\oplus (-1,-1,-1,-1,1) \\oplus (2,2,0,1,-1)\n",
    "$$\n",
    "The first four line bundles have charges $q_i^j \\in \\{-1,0,1\\}$ and the last line bundle fixes $c_1(V) =0$.\n",
    "We will need to perform quite some actions in order to match the realistic configuration from above. Starting with the first line bundle we increase the first charge by one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_obs, r, done, info = env.step(0)\n",
    "print('The new observation is: \\n {}'.format(new_obs))\n",
    "print('The reward for our action is: {}'.format(r))\n",
    "print('Did we find a model? {}'.format(done))\n",
    "print('Do you want to tell us something about this model? \\n {}'.format(info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first charge has increased by one. This was compensated by a change in the first charge of the fifth line bundle. We note that this is not a very good configuration, as we recieve a negative reward of -0.6. We already fail the first reward check, since the third line bundle is semipositive and thus can not have slope zero anywhere in the KÃ¤hler cone.\n",
    "\n",
    "The next actions we have to do in order to reach our model are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [0,1,22,23,4,5,27,8,34,15,17,17,18,39]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, an action value of $3+4\\cdot h^{1,1}$ decreases the 3+1th charge. Lets iterate over the actions and observe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in actions:\n",
    "    new_obs, r, done, info = env.step(a)\n",
    "    print('The new observation is: \\n {}'.format(new_obs))\n",
    "    print('The reward for our action is: {}'.format(r))\n",
    "    print('Did we find a model? {}'.format(done))\n",
    "    print('Do you want to tell us something about this model? \\n {}'.format(info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and in fact we made it. Congratulation a first model has been found! It is interesting how highly nonlinear our space is, changing a single charge resulted in a change of reward from -0.4 to 11020102.0 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3C agent\n",
    "\n",
    "In this section we will build a simple A3C agent using Chainer RL. [A3C agents](https://arxiv.org/abs/1602.01783) are defined by a policy and a state value function which are being approximated by two neural networks sharing the first layers of weights.\n",
    "\n",
    "First we import some useful Chainer functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainerrl\n",
    "import chainer.functions as F\n",
    "from chainerrl.agents import a3c\n",
    "from chainerrl import experiments\n",
    "import chainer.links as L\n",
    "from chainerrl import misc\n",
    "from chainerrl.optimizers.nonbias_weight_decay import NonbiasWeightDecay\n",
    "from chainerrl.optimizers import rmsprop_async\n",
    "from chainerrl import policy\n",
    "from chainerrl import v_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a simple NN with two hidden layers and each having $n_{hidden}$ Neurons using ReLU activation functions. We can define such a NN by chaining the respective layers together into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Body(chainer.Chain):\n",
    "\n",
    "    def __init__(self, obs_size, n_hidden_channels):\n",
    "        \n",
    "        # save input and output length in its own variables\n",
    "        self.n_output_channels = n_hidden_channels\n",
    "        self.n_input_channels = obs_size\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        with self.init_scope():\n",
    "            # input layer\n",
    "            self.l0 = L.Linear(obs_size, n_hidden_channels)\n",
    "            # first hidden\n",
    "            self.l1 = L.Linear(n_hidden_channels, n_hidden_channels)\n",
    "            # second hidden\n",
    "            self.l2 = L.Linear(n_hidden_channels, n_hidden_channels)\n",
    "\n",
    "    def __call__(self, obs):\n",
    "        # we feed the observation through the linear layers\n",
    "        # and apply the non linearity via a ReLU function. \n",
    "        h = F.relu(self.l0(obs))\n",
    "        h = F.relu(self.l1(h))\n",
    "        return F.relu(self.l2(h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the body we can complete the A3Cagent by introducing a softmax layer for the policy and another linear layer with one outgoing node for the state value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3Cagent(chainer.ChainList, a3c.A3CModel):\n",
    "\n",
    "    def __init__(self, n_input, n_actions, n_hidden):\n",
    "\n",
    "        self.head = Body(n_input, n_hidden)\n",
    "        self.pi = policy.FCSoftmaxPolicy(self.head.n_output_channels, n_actions)\n",
    "        self.v = v_function.FCVFunction(self.head.n_output_channels)\n",
    "        super().__init__(self.head, self.pi, self.v)\n",
    "\n",
    "    def pi_and_v(self, state):\n",
    "        out = self.head(state)\n",
    "        return self.pi(out), self.v(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now in a position to define our A3C model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters\n",
    "obs_size = 5*M.len\n",
    "n_actions = 2*4*M.len\n",
    "n_hidden = 100\n",
    "\n",
    "# define the model\n",
    "model = A3Cagent(obs_size, n_actions, n_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having set up the model we can feed it into the [A3C chainer class](https://github.com/chainer/chainerrl/blob/master/chainerrl/agents/a3c.py). This one requires a couple more arguments however. \n",
    "\n",
    "We feed it the following hyperparameters:\n",
    "\n",
    "1. $\\gamma$ the discount factor for accumulated reward\n",
    "2. $t_{max}$ the number of time steps afet which the weights are updated\n",
    "3. the optimization algorithm; we use RMsprop with learning rate $lr$ and momentum $\\alpha$ and numerical stability parameter $\\epsilon$\n",
    "4. $\\beta$ the regularization parameter sitting in front of the entropy loss term\n",
    "5. $\\phi$ a function applied on the observation space.\n",
    "6. gradient clipping $gc$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 1\n",
    "gamma = 0.95\n",
    "tmax = 5\n",
    "lr = 0.0001\n",
    "alpha = 0.99\n",
    "epsilon = 0.00001\n",
    "opt = rmsprop_async.RMSpropAsync(lr=lr, eps=epsilon, alpha=alpha)\n",
    "gc = 20\n",
    "phi = lambda x: x.astype(np.float32, copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we define the A3C agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = a3c.A3C(model, opt, t_max=tmax, gamma=gamma,\n",
    "                    beta=beta, phi=phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and setup the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.setup(model)\n",
    "opt.add_hook(chainer.optimizer.GradientClipping(gc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a last step, we let the agent train using the Chainer experiment [*train_agent_async*](https://github.com/chainer/chainerrl/blob/master/chainerrl/experiments/train_agent_async.py).\n",
    "This one requires\n",
    "\n",
    "1. a function that creates the gym environments for every worker with different seeds \n",
    "2. the number of training steps\n",
    "3. the number of workers/threads used for training\n",
    "4. the number of evaluation runs and interval\n",
    "5. the number of maximal steps per Episode\n",
    "6. a decay hook for the learning rate\n",
    "\n",
    "and more ... ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the auxillary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_setter(env, agent, value):\n",
    "    agent.optimizer.lr = value\n",
    "    \n",
    "def make_env(process_idx, test):\n",
    "    process_seed = process_seeds[process_idx]\n",
    "    env_seed = 2 ** 31 - 1 - process_seed if test else process_seed\n",
    "    env = gym.make('CICY-v1')\n",
    "    env.seed(int(env_seed))\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set the remaining hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "nsteps = 10**6\n",
    "eval_n_runs = 10\n",
    "eval_interval = 50000\n",
    "max_episode_len = 200\n",
    "threads = multiprocessing.cpu_count()\n",
    "lr_decay_hook = experiments.LinearInterpolationHook(nsteps, lr, 0, lr_setter)\n",
    "outdir = 'results'\n",
    "process_seeds = np.arange(threads) + seed * threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you run the next cell, make sure you double checked all the previous hyperparameters and make sure they are compatible with your system (e.g. number of threads ..). \n",
    "\n",
    "Furthermore, if you want to know the number of models found during training the training loop in ChainerRL has to be changed according to the [readme](https://github.com/robin-schneider/gymCICY)\n",
    "\n",
    "Training will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = experiments.train_agent_async(\n",
    "        agent=agent,\n",
    "        outdir=outdir,\n",
    "        processes=threads,\n",
    "        make_env=make_env,\n",
    "        steps=nsteps,\n",
    "        eval_interval=eval_interval,\n",
    "        eval_n_episodes=eval_n_runs,\n",
    "        max_episode_len=max_episode_len,\n",
    "        global_step_hooks=[lr_decay_hook])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can load the scores and see how our agents performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We parse through the results folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(outdir):\n",
    "    if 'scores.txt' in files:\n",
    "        scores += [pd.read_csv(os.path.join(root, 'scores.txt'), delimiter='\\t')]\n",
    "    if 'model_info' in files:\n",
    "        tmp_models = []\n",
    "        with open(os.path.join(root, 'model_info')) as f:\n",
    "            fline = f.readlin()\n",
    "            tmp_models += [json.loads(fline)]\n",
    "        models += [tmp_models]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we make some plots. Here is data collected in the evaluation runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores[0].plot(x='steps', y='mean'))\n",
    "print(scores[0].plot(x='steps', y='average_value'))\n",
    "print(scores[0].plot(x='steps', y='average_entropy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and more interestingly we plot the models found. This step assumes that the ChainerRL training loop has been changed previously according to the [readme](https://github.com/robin-schneider/gymCICY)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(models) != 0:\n",
    "    nmodels = np.arange(len(models[0]))\n",
    "    time_steps = np.array([m['gt'] for m in models[0]])\n",
    "    plt.title('6777 - flipping')\n",
    "    plt.xlabel('steps')\n",
    "    plt.ylabel('\\# of models')\n",
    "    plt.plot()\n",
    "    plt.legen(loc='best')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
